{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import struct\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .csv -> .param 转换器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_csv_to_param():\n",
    "    with open(netName+'.param','w') as p: \n",
    "        with open(netName+'.csv','r') as f:\n",
    "            f_csv = csv.reader(f)\n",
    "\n",
    "            blobs = []\n",
    "            outputcontent = ''\n",
    "            row_index = 0\n",
    "            for row in f_csv:   ## read rows in csv\n",
    "                # normal lines <layertype   layername   inputNum outputNum inputBlob outputBlob params>\n",
    "                col_index = 0\n",
    "                for col in row:\n",
    "                    if col_index == 0:                # layertype\n",
    "                        outputcontent += '%-16s ' % (col)\n",
    "                    elif col_index == 1:              # layername\n",
    "                        outputcontent += '%-32s ' % (col)\n",
    "                    elif col_index == 2:              # inputNum\n",
    "                        inputBlobNum = int(col)\n",
    "                        outputcontent += ' %s' % (col)\n",
    "                    elif col_index == 3:              # outputNum\n",
    "                        outputBlobNum = int(col)\n",
    "                        outputcontent += ' %s' % (col)\n",
    "                    elif col_index > 3 and col_index <= 3+inputBlobNum+outputBlobNum:  # inputBlob outputBlob\n",
    "                        blobs.append(col)\n",
    "                        outputcontent += ' %s' % (col)\n",
    "                    else:                             # params\n",
    "                        outputcontent += ' %s' % (col)\n",
    "                    col_index += 1\n",
    "                outputcontent += '\\n'\n",
    "                row_index += 1  ## end of read rows in csv\n",
    "            outputcontent = '7767517\\n'+'%s '%(row_index)+'%s\\n'%(len(list(set(blobs))))+outputcontent\n",
    "            p.write(outputcontent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调试用 np array保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for debug\n",
    "def save_imagedata(npa,filename):   \n",
    "    import os\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except:\n",
    "        pass\n",
    "    with open(filename,'w+') as f:\n",
    "        temp = npa.flatten()\n",
    "\n",
    "        print(npa.shape)\n",
    "        index = 1\n",
    "        sum = 0\n",
    "        for i in temp.tolist():\n",
    "            f.write(str(i)[0:8])\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. import tflite json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonpath = \"/home/gx/myproj/flatbuffers/im14_without_q_att.json\"\n",
    "with open(jsonpath,'r') as f_json:\n",
    "    model = json.load(f_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 层权重bin写入器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM o-h-w-i(tflite) TO o-i-h-w(ncnn)\n",
    "def fix_weight_order(a,shape):\n",
    "    temp = np.array(a).reshape(shape+(4,))\n",
    "    print(temp.transpose(0,3,1,2,4).shape)\n",
    "    return temp.transpose(0,3,1,2,4).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weight & bias data from tflite-json file to ncnn-bin file\n",
    "def save_data_to_bin(bufferIndex, shape=(), savefilter=True, append=True):\n",
    "    bufferData = model['buffers'][bufferIndex]['data']\n",
    "    flag = 'ab' if append else 'wb'\n",
    "    with open(netName+'.bin',flag) as f:\n",
    "        if savefilter:\n",
    "            i = 0x00000000   # flag-structure: weight data need to save as float type\n",
    "            f.write(struct.pack('<I',i))\n",
    "            bufferData = fix_weight_order(bufferData,shape)\n",
    "        for i in bufferData: # sava bufferdata, no need flag-structure\n",
    "            f.write(struct.pack('B',i))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 层csv写入器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_row_to_csv(appendList):\n",
    "    with open(netName +'.csv','a', newline = \"\") as f:\n",
    "        csv_writer = csv.writer(f, dialect = \"excel\")\n",
    "        csv_writer.writerow(appendList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conv(name, indexTfLite, inputBlob, outputBlob):\n",
    "    \n",
    "    op = model['subgraphs'][0]['operators']\n",
    "    tensor = model['subgraphs'][0]['tensors']\n",
    "    \n",
    "    filterTensor = op[indexTfLite]['inputs'][1]\n",
    "    biasTensor = op[indexTfLite]['inputs'][2]\n",
    "\n",
    "    if 'dilation_w_factor' in op[indexTfLite]['builtin_options'].keys():\n",
    "        dilation_w_factor = op[indexTfLite]['builtin_options']['dilation_w_factor']\n",
    "    else:\n",
    "        dilation_w_factor = '1'\n",
    "        \n",
    "    if 'stride_w' in op[indexTfLite]['builtin_options'].keys():\n",
    "        stride_w = op[indexTfLite]['builtin_options']['stride_w']\n",
    "    else:\n",
    "        stride_w = '1'\n",
    "\n",
    "    fliterShape = tuple(tensor[filterTensor]['shape'])\n",
    "    \n",
    "    param = ['0=' + str(fliterShape[0]),                    # num_output\n",
    "             '1=' + str(fliterShape[1]),                    # kernel_w   （kernel_h default）\n",
    "             '2=' + str(dilation_w_factor),                 # rate\n",
    "             '3=' + str(stride_w),                          # stride\n",
    "             '4=-233',                                      # same\n",
    "             '5=1',                                         # has bias\n",
    "             '6=' + str(reduce(lambda x,y:x*y,fliterShape)) # weight\n",
    "            ]\n",
    "    \n",
    "    append_row_to_csv(['Convolution']+[name]+['1','1']+[inputBlob,name]+param)\n",
    "    save_data_to_bin(tensor[filterTensor]['buffer'],fliterShape,savefilter=True)  # save filter's buffer (weight)\n",
    "    save_data_to_bin(tensor[biasTensor]['buffer'],savefilter=False)   # save bias's   buffer \n",
    "    return outputBlob\n",
    "\n",
    "def save_conv2(name, inputBlob, filterBlob, outputBlob,kernel_w,num_output,weight_size,stride=1):\n",
    "\n",
    "    param = ['0=' + str(num_output),                        # num_output\n",
    "             '1=' + str(kernel_w),                          # kernel_w   （kernel_h default）\n",
    "             '2=1',                                         # rate\n",
    "             '3=' + str(stride),                            # stride\n",
    "             '4=-233',                                      # same\n",
    "             '5=0',                                         # has no bias\n",
    "             '6=' + str(weight_size)                        # weight_size\n",
    "            ]\n",
    "    \n",
    "    append_row_to_csv(['Convolution2']+[name]+['2','1']+[inputBlob,filterBlob,name]+param)\n",
    "    return outputBlob\n",
    "\n",
    "def save_conv3(name, inputBlob, outputBlob,kernel_w,num_output,weight_size):\n",
    "\n",
    "    param = ['0=' + str(num_output),                        # num_output\n",
    "             '1=' + str(kernel_w),                          # kernel_w   （kernel_h default）\n",
    "             '2=1',                                         # rate\n",
    "             '3=1',                                         # stride\n",
    "             '4=-233',                                      # same\n",
    "             '5=0',                                         # has no bias\n",
    "             '6=' + str(weight_size)                        # weight_size\n",
    "            ]\n",
    "    \n",
    "    append_row_to_csv(['Convolution3']+[name]+['1','1']+[inputBlob,outputBlob]+param)\n",
    "    return outputBlob\n",
    "\n",
    "def save_slice(name, inputBlob, outputBlob1, outputBlob2):\n",
    "    append_row_to_csv(['Slice']+[name]+['1','2']+[inputBlob,outputBlob1,outputBlob2]+['-23300=2,-233,-233','1=0'])\n",
    "    return outputBlob1, outputBlob2\n",
    "\n",
    "def save_interp(name, inputBlob, outputBlob, scale=2.0):    # ResizeNearestNeighbor  align corner\n",
    "    append_row_to_csv(['Interp']+[name]+['1','1']+[inputBlob,outputBlob]+['0=4','1='+str(scale),'2='+str(scale)]) \n",
    "    return outputBlob\n",
    "\n",
    "def save_normalize(name, inputBlob, outputBlob):            # custom normalize\n",
    "    append_row_to_csv(['Normalize_sp']+[name]+['1','1']+[inputBlob,outputBlob]) \n",
    "    return outputBlob\n",
    "\n",
    "def save_permute(name, inputBlob, outputBlob,ordertype):\n",
    "    append_row_to_csv(['Permute']+[name]+['1','1']+[inputBlob,outputBlob]+['0='+str(ordertype)])\n",
    "    return outputBlob\n",
    "\n",
    "def save_reshape(name, inputBlob, outputBlob,w,h,c):\n",
    "    append_row_to_csv(['Reshape']+[name]+['1','1']+[inputBlob,outputBlob]+['0='+str(w)]+['1='+str(h)]+['2='+str(c)])\n",
    "    return outputBlob\n",
    "\n",
    "def save_elu(name, inputBlob, outputBlob):\n",
    "    append_row_to_csv(['ELU']+[name]+['1','1']+[inputBlob,outputBlob]+['0=1.0'])\n",
    "    return outputBlob\n",
    "    \n",
    "def save_relu(name, inputBlob, outputBlob):\n",
    "    append_row_to_csv(['ReLU']+[name]+['1','1']+[inputBlob,outputBlob])\n",
    "    return outputBlob\n",
    "    \n",
    "def save_tanh(name, inputBlob, outputBlob):\n",
    "    append_row_to_csv(['TanH']+[name]+['1','1']+[inputBlob,outputBlob])\n",
    "    return outputBlob\n",
    "    \n",
    "def save_sigmoid(name, inputBlob, outputBlob):\n",
    "    append_row_to_csv(['Sigmoid']+[name]+['1','1']+[inputBlob,outputBlob])\n",
    "    return outputBlob\n",
    "\n",
    "def save_mul(name, inputBlob1, inputBlob2, outputBlob):\n",
    "    append_row_to_csv(['BinaryOp']+[name]+['2','1']+[inputBlob1,inputBlob2,outputBlob]+['0=2'])\n",
    "    return outputBlob\n",
    "\n",
    "def save_mul_s(name, inputBlob, outputBlob):\n",
    "    append_row_to_csv(['BinaryOp']+[name]+['1','1']+[inputBlob,outputBlob]+['0=2','1=1','2=10.0'])\n",
    "    return outputBlob\n",
    "\n",
    "def save_add(name, inputBlob1, inputBlob2, outputBlob):\n",
    "    append_row_to_csv(['BinaryOp']+[name]+['2','1']+[inputBlob1,inputBlob2,outputBlob]+['0=0'])\n",
    "    return outputBlob\n",
    "\n",
    "def save_sub(name, inputBlob1, inputBlob2, outputBlob):\n",
    "    append_row_to_csv(['BinaryOp']+[name]+['2','1']+[inputBlob1,inputBlob2,outputBlob]+['0=1'])\n",
    "    return outputBlob\n",
    "\n",
    "def save_div_s(name, inputBlob, outputBlob):\n",
    "    append_row_to_csv(['BinaryOp']+[name]+['1','1']+[inputBlob,outputBlob]+['0=3','1=1','2=4.0'])\n",
    "    return outputBlob\n",
    "\n",
    "def save_split(name, inputBlob, outputBlob1, outputBlob2):\n",
    "    append_row_to_csv(['Split']+[name]+['1','2']+[inputBlob,outputBlob1,outputBlob2])\n",
    "    return outputBlob1, outputBlob2\n",
    "\n",
    "def save_concat(name, inputBlob1, inputBlob2, outputBlob):\n",
    "    append_row_to_csv(['Concat']+[name]+['2','1']+[inputBlob1,inputBlob2,outputBlob]+['0=0']) # 0 dim concat 1 h 2 w\n",
    "    return outputBlob\n",
    "\n",
    "def save_input(name, d0, d1, d2):\n",
    "    append_row_to_csv(['Input']+[name]+['0','1']+[name]+['0='+str(d0)]+['1='+str(d1)]+['2='+str(d2)])\n",
    "    return name\n",
    "\n",
    "def save_memorydata(name, d0, d1, d2):\n",
    "    append_row_to_csv(['MemoryData']+[name]+['0','1']+[name]+['0='+str(d0)]+['1='+str(d1)]+['2='+str(d2)])\n",
    "    return name\n",
    "\n",
    "def save_ert(name,inputBlob, outputBlob, sizes, strides):\n",
    "    append_row_to_csv(['Ert']+[name]+['1','1']+[inputBlob,outputBlob]+['0='+str(sizes),'1='+str(strides)]) \n",
    "    return outputBlob\n",
    "\n",
    "def save_exc(name,inputBlob, outputBlob,exc_type):\n",
    "    append_row_to_csv(['Exc']+[name]+['1','1']+[inputBlob,outputBlob]+['0='+str(exc_type)]) \n",
    "    return outputBlob\n",
    "\n",
    "def save_reducemean(name,inputBlob, outputBlob):\n",
    "    append_row_to_csv(['Reducemean']+[name]+['1','1']+[inputBlob,outputBlob]) \n",
    "    return outputBlob\n",
    "\n",
    "def save_softmax(name,inputBlob, outputBlob,axis,fixbug0):\n",
    "    append_row_to_csv(['Softmax']+[name]+['1','1']+[inputBlob,outputBlob]+['0='+str(axis),'1='+str(fixbug0)]) \n",
    "    return outputBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 卷积unit 分类型处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conv_unit(unitType, indexTfLite, inputBlob, deconv, stage=''):\n",
    "    \n",
    "    def N(name):\n",
    "        return stage+'/'+name+'_'+str(indexTfLite)\n",
    "    \n",
    "    x = inputBlob\n",
    "    \n",
    "    if deconv:\n",
    "        x = save_interp(('interp'), x, N('interp'))\n",
    "        \n",
    "    x = save_conv(N('conv2d'),indexTfLite, x, N('conv2d'))\n",
    "    \n",
    "    if unitType == 'TanH':\n",
    "        x = save_tanh(N('tanh'),x,N('tanh'))\n",
    "    else:  \n",
    "        x,y = save_slice(N('slice'), x, N('conv2d')+'_A', N('conv2d')+'_B')\n",
    "\n",
    "        if unitType == 'ELU':\n",
    "            x = save_elu(N('elu'),x,N('elu'))\n",
    "        if unitType == 'ReLU':\n",
    "            x = save_relu(N('relu'),x,N('relu'))\n",
    "\n",
    "        y = save_sigmoid(N('sigmoid'),y,N('sigmoid'))\n",
    "        x = save_mul(N('mul'),x,y,N('mul'))   \n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "netName = 'mynet_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net():\n",
    "    \n",
    "    import os \n",
    "    try:\n",
    "        os.remove(netName+'.csv')\n",
    "        os.remove(netName+'.bin')\n",
    "    except:\n",
    "        pass\n",
    "    #########   save memorydata ########\n",
    "    a = [1]\n",
    "    with open(netName+'.bin','wb') as f:\n",
    "        for i in a:\n",
    "            print(struct.pack('<f',i))\n",
    "            f.write(struct.pack('<f',i))\n",
    "    ####################################        \n",
    "\n",
    "    \n",
    "    x = save_input('data',512,680,3)\n",
    "    y = save_input('mask',512,680,3)\n",
    "    z = save_input('mask_single',512,680,1)\n",
    "    x1,x2 = save_split('b_i/split',x,'b_i_1','b_i_2')\n",
    "    y1,y2 = save_split('mask/split',y,'mask_1','mask_2')\n",
    "    y11,y12 = save_split('mask/split_1',y1,'mask_1_1','mask_1_2')\n",
    "    y21,y22 = save_split('mask/split_2',y2,'mask_2_1','mask_2_2') \n",
    "    m = save_memorydata('one',1,0,0)\n",
    "    y11 = save_sub('sub', m, y11, 'sub')\n",
    "    y111,y112 = save_split('one_sub_mask/split',y11,'sub_1','sub_2')\n",
    "    i = save_input('inpaint',512,680,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x00\\x00\\x80?'\n",
      "(48, 5, 5, 5, 4)\n",
      "(96, 24, 3, 3, 4)\n",
      "(96, 48, 3, 3, 4)\n",
      "(192, 48, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(96, 96, 3, 3, 4)\n",
      "(96, 48, 3, 3, 4)\n",
      "(48, 48, 3, 3, 4)\n",
      "(24, 24, 3, 3, 4)\n",
      "(3, 12, 3, 3, 4)\n",
      "(48, 3, 5, 5, 4)\n",
      "(48, 24, 3, 3, 4)\n",
      "(96, 24, 3, 3, 4)\n",
      "(96, 48, 3, 3, 4)\n",
      "(192, 48, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(48, 3, 5, 5, 4)\n",
      "(48, 24, 3, 3, 4)\n",
      "(96, 24, 3, 3, 4)\n",
      "(192, 48, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(192, 192, 3, 3, 4)\n",
      "(192, 96, 3, 3, 4)\n",
      "(96, 96, 3, 3, 4)\n",
      "(96, 48, 3, 3, 4)\n",
      "(48, 48, 3, 3, 4)\n",
      "(24, 24, 3, 3, 4)\n",
      "(3, 12, 3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# 辅助网络\n",
    "init_net()  # also flash param & bin file \n",
    "\n",
    "# 分级主干网络 stage1\n",
    "stage1 = [13,20,25,30,35, 40,45,50,55,60, 65,70,76,81,87, 92,97]\n",
    "index = 1\n",
    "x = 'inpaint'\n",
    "for i in stage1:\n",
    "    unitType = 'TanH' if index == 17 else 'ELU'\n",
    "    deconv = True if index in [13,15] else False\n",
    "    x = save_conv_unit(unitType,i,x,deconv,'stage1')\n",
    "    index += 1\n",
    "    \n",
    "# 辅助结构\n",
    "x = save_mul('stage1/output_mask2_mul', 'mask_2_1', x, 'stage1/output_mask2_mul')\n",
    "bm = save_mul('bi_submask', 'sub_2', 'b_i_2', 'bi_submask')\n",
    "x = save_add('stage1/output_bi_add', bm, x, 'stage1/output_bi_add')\n",
    "x,y = save_split('stage2',x,'stage2_A','stage2_B')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 分级主干网络 stage2_A\n",
    "stage2_A = [101,111,121,131,141, 151,162,173,181,186]\n",
    "index = 1\n",
    "for i in stage2_A:\n",
    "    x = save_conv_unit('ELU',i,x,False,'stage2_A')\n",
    "    index += 1\n",
    "\n",
    "# 分级主干网络 stage2_B\n",
    "stage2_B = [102,112,122,132,142, 152]   # from pmconv1 to pmconv6\n",
    "index = 1\n",
    "for i in stage2_B:\n",
    "    unitType = 'ReLU' if index == 6 else 'ELU'\n",
    "    y = save_conv_unit(unitType,i,y,False,'stage2_B')\n",
    "    index += 1\n",
    "\n",
    "w,raw_w = save_split('attention_start',y,'att','main')\n",
    "    \n",
    "### attention net\n",
    "# mask branch\n",
    "att_mask = save_interp('att_mask_interp', 'mask_single', 'att_mask_interp', scale=0.25)\n",
    "att_mask = save_interp('att_mask_interp1', att_mask, 'att_mask_interp1', scale=0.5)\n",
    "att_mask = save_ert('ert_mask',att_mask,'ert_mask',3,1)\n",
    "att_mask = save_reducemean('reducemean',att_mask, 'reducemean')\n",
    "att_mask1,att_mask2 = save_split('reducemean_split',att_mask,'reducemean_A','reducemean_B')\n",
    "\n",
    "# w branch\n",
    "interp = save_interp('w_interp', w, 'w_interp', scale=0.5)\n",
    "convi,convf = save_split('w',interp,'conv_input','conv_filter')\n",
    "ert_w = save_ert('ert_w',convf,'ert_w',3,1)\n",
    "norm = save_normalize('norm',ert_w,'norm')\n",
    "y = save_conv2('w_conv', convi, norm, 'w_conv',3,5440,3*3*96*5440)\n",
    "y = save_reshape('reshape1', y, 'reshape1',5440,5440,1)\n",
    "y = save_conv3('eyeconv1', y, 'eyeconv1',3,1,9)\n",
    "y = save_exc('exc1',y, 'exc1',0)\n",
    "y = save_conv3('eyeconv2', y, 'eyeconv2',3,1,9)\n",
    "y = save_exc('exc2',y, 'exc2',1)\n",
    "y = save_reshape('reshape2', y, 'reshape2',85,64,5440)\n",
    "att_mask1 = save_mul_s('mul10',att_mask1,'mul10')\n",
    "y = save_mul('reducemean_A_reshape2', att_mask1, y, 'mul_mean_reshape')\n",
    "y = save_softmax('softmax',y, 'softmax',2,2)\n",
    "y = save_mul('reducemean_B_softmax', att_mask2, y, 'mul_mean_softmax')\n",
    "y = save_interp('transpose_interp_zero', y, 'transpose_interp_zero', scale=2)\n",
    "\n",
    "# raw_w branch\n",
    "ert_raw_w = save_ert('ert_raw_w',raw_w,'ert_raw_w',4,2)\n",
    "\n",
    "# transpose_conv\n",
    "y = save_conv2('transpose_conv', y, ert_raw_w, 'transpose_conv',4,96,4*4*96*5440,2)\n",
    "y = save_div_s('div',y,'div')\n",
    "\n",
    "\n",
    "# 分级主干网络 stage2_B\n",
    "y = save_conv_unit('ELU',161,y,False,'stage2_B') # pmconv9\n",
    "y = save_conv_unit('ELU',170,y,False,'stage2_B') # pmconv10\n",
    "    \n",
    "# 拼接 stage2_A 和 stage2_B\n",
    "x = save_concat('concat2',x,y,'concat2')\n",
    "\n",
    "# 分级主干网络 stage2_C\n",
    "stage2_C = [192,197,203,208,214, 219,224]\n",
    "index = 1\n",
    "for i in stage2_C:\n",
    "    unitType = 'TanH' if index == 7 else 'ELU'\n",
    "    deconv = True if index in [3,5] else False\n",
    "    x = save_conv_unit(unitType,i,x,deconv,'stage2_C')\n",
    "    index += 1\n",
    "    \n",
    "# 辅助结构\n",
    "pm1 = save_mul('mul', 'b_i_1', 'sub_1', 'mul')\n",
    "pm2 = save_mul('mul_1', 'mask_1_2', x, 'mul_1')\n",
    "x = save_add('add', pm1, pm2, 'add')\n",
    "\n",
    "# 生成最终网络结构param文件\n",
    "trans_csv_to_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
